% "THE BEER-WARE LICENSE":
% Daniel Oom wrote this file. As long as you retain this notice you
% can do whatever you want with this stuff. If we meet some day, and you think
% this stuff is worth it, you can buy me a beer in return Daniel Oom

% (fold) Header
\documentclass[10pt,landscape]{article}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{ifthen}
\usepackage{mathtools}  
\usepackage{multicol}

\newcommand{\expect}[1]{\mathrm{E} \left[ {#1} \right]}
\newcommand{\cidist}[1]{{#1}_{\frac{\alpha}{2}}}
\newcommand{\pisigma}[0]{\frac{1}{\sqrt{2 \pi \sigma^2}}}

\pdfinfo{
  /Title (sheet.pdf)
  /Creator (latex)
  /Producer (pdflatex)
  /Author (Daniel Oom)
  /Subject (Mathmatical Statistics)
  /Keywords (math, statistics, cheat sheet)}

\mathtoolsset{showonlyrefs}

\geometry{top=1.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm}

% Header
\fancyhf{}
\pagestyle{fancy}
\lhead{MatStat Cheat Sheet rev DEVELOPMENT}
\rhead{By Daniel Oom, github.com/daoo/matstat}
\setlength{\headsep}{8pt}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}
                                {-1ex plus -.5ex minus -.2ex}
                                {0.5ex plus .2ex}
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}
                                {-1explus -.5ex minus -.2ex}
                                {0.5ex plus .2ex}
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}
                                {-1ex plus -.5ex minus -.2ex}
                                {1ex plus .2ex}
                                {\normalfont\small\bfseries}}
\makeatother

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{4}

% multicol parameters
\setlength{\columnseprule}{0.25pt}
%\setlength{\premulticols}{1pt}
%\setlength{\postmulticols}{1pt}
%\setlength{\multicolsep}{1pt}
%\setlength{\columnsep}{2pt}
% (end)

% (fold) Probability
\section{Probability}
\begin{math}
  P(\emptyset) = 0, P(\Omega) = 1,
  P(A^c) = 1 - P(A),
  A^c \cup A = \Omega
\end{math} \\[2pt]
\begin{math}
  P(A^c) + P(A) = P(A^c \cup A) = P(\Omega) = 1
\end{math} \\[2pt]
\begin{math}
  P(A \cup B) = P(A) + P(B) - P(A \cap B)
\end{math} \\[2pt]
\begin{math}
  P(A) = P(A \cap B^c) + P(A \cap B) + P(A^c \cap B) + P(A \cap B) = P(A \cup B) + P(A \cap B)
\end{math} \\[2pt]
\begin{math}
  P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{math} \\[0pt]
A, B independent if
\begin{math}
  P(A \cap B) = P(A) \cdot P(B)
\end{math} \\[2pt]
Bayes: 
\begin{math}
  P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\end{math} \\[2pt]
Total Probability Law:
\begin{math}
  P(B) = \sum_{i} \left[P(B|A_i) \cdot P(A_i)\right]
\end{math} \\[2pt]
% (end)
% (fold) Calculus
\section{Calculus}
\begin{math}
  \frac{d}{dx} f(x)g(x) = f'(x)g(x) + f(x)g'(x)
\end{math} \\[2pt]
% (end)
% (fold) Random Variables
\section{Random Variables}
\begin{math}
  f(x) = P(X = x), F(x) = P(X \leq x)
\end{math}
\begin{math}
  \sum_{x \in \Omega}{f(x)} = 1
\end{math}

% (fold) Variability
\subsection{Variability}
\begin{math}
  \expect{h(X)} = \sum_{all \, x} h(x) f(x)
\end{math} \\[2pt]
\begin{math}
  \expect{a} = a, \expect{aX} = a\expect{X}
\end{math} \\[2pt]
\begin{math}
  \expect{X + Y} = \expect{X} + \expect{Y}
\end{math} \\[2pt]
\begin{math}
  Var(X) = \sum_{all \, x} f(x) (x - \mu)^2
\end{math} \\[2pt]
\begin{math}
  Var(X) = \expect{X^2} - (\expect{X})^2 
\end{math} \\[2pt]
\begin{math}
  Var(a) = 0, Var(aX) = a^2 Var(X) 
\end{math} \\[2pt]
\begin{math}
  Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)
\end{math} \\[2pt]
\begin{math}
  E \left[ \frac{X - \mu}{\sigma} \right] = 0, Var \left( \frac{X - \mu}{\sigma} \right) = 1
\end{math}
% (end)
% (fold) Discrete Random Variables
\subsection{Discrete Random Variables}
\begin{math}
  f(x) = F(x) - F(x - 1)
\end{math}
% (end)
% (fold) Continuous Random Variables
\subsection{Continuous Random Variables}
\begin{math}
  P[X = a] = P[X = b] = 0 
  \Rightarrow
  P[a \leq X \leq b] = P[a < X < b]
\end{math} \\[2pt]
\begin{math}
  f(x) = F'(x)
\end{math}
% (end)
% (fold) Multivariate Random Variables
\subsection{Multivariate Random Variables}
\begin{math}
  f_{XY}(x, y) = P[X = x \text{ and } Y = y]
\end{math} \\[2pt]
\begin{math}
  f_{X}(x) = \sum_{all \, y}{f_{XY}(x,y)}
\end{math} \\[2pt]
\begin{math}
  f_{Y}(y) = \sum_{all \, x}{f_{XY}(x,y)}
\end{math} \\[4pt]
\begin{math}
  Cov(X, Y) = \sigma_{XY} = \expect{(X - \expect{X}) \cdot (Y - \expect{Y})} = \expect{XY} - \expect{X} \cdot \expect{Y}
\end{math} \\[2pt]
\begin{math}
  Cov(X, a) = 0, Cov(X, X) = Var(x)
\end{math} \\[2pt]
\begin{math}
  Cov(X, Y) = Cov(Y, X)
\end{math} \\[2pt]
\begin{math}
  Cov(aX, bY) = abCov(X, Y)
\end{math} \\[2pt]
\begin{math}
  Cov(X + a, Y + b) = Cov(X, Y)
\end{math} \\[2pt]
\begin{math}
  Corr(X, Y) = \varphi_{XY} = \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}}
\end{math}
\begin{math}
  X = Y \Rightarrow Corr(X, Y) = 1
\end{math}
% (end)
% (fold) Independent Random Variables
\subsection{Independent Random Variables}
\begin{math}
  \expect{XY} = \expect{X} \cdot \expect{Y}
\end{math} \\[2pt]
\begin{math}
  Var(X + Y) = Var(X) + Var(Y)
\end{math} \\[2pt]
\begin{math}
  Cov(X, Y) = 0 \Leftrightarrow Corr(X, Y) = 0
\end{math}
% (end)
% (end)
% (fold) Distributions
\section{Distributions}
\subsection{Geometric Distribution}
\begin{math}
  X \sim Geom(p)
\end{math} \\[2pt]
\begin{math}
  f(x) = (1 - p)^{x - 1}p
\end{math} \\[2pt]
\begin{math}
  F(x) = 1 - (1 - p)^{\lfloor x \rfloor}
\end{math} \\[2pt]
\begin{math}
  \expect{X} = \frac{1}{p},
  Var(X) = \frac{1 - p}{p^2}
\end{math} \\[0pt]

\subsection{Binomial Distribution}
Fixed number, \(n\), of Bernoulli trials, independent trials with equal probability \(p\).
\begin{math}
  X \sim Bin(n, p)
\end{math} \\[2pt]
\begin{math}
  f(x) = \binom{n}{k} p^k (1 - p)^{n - k}
\end{math} \\[2pt]
\begin{math}
  \expect{X} = np, Var(X) = np(1 - p)
\end{math} \\[0pt]

\subsection{Continuous Uniform Distribution}
\begin{math}
  X \sim U(a, b)
\end{math} \\[2pt]
\begin{math}
  f(x) = \frac{1}{b - a},
  F(x) = \frac{x - a}{b - a}
\end{math} \\[2pt]
\begin{math}
  \expect{X} = \frac{1}{2}(a + b),
  Var(X) = \frac{1}{12}(b - a)^2
\end{math} \\[0pt]

\subsection{Normal Distribution}
\begin{math}
  X \sim N(\mu, \sigma^2)
\end{math} \\[0pt]
\begin{math}
  f(x) = \pisigma \cdot exp(-\frac{(x - \mu)^2}{2 \sigma^2})
\end{math} \\[0pt]

\subsection{Standard Normal Distribution}
\begin{math}
  X \sim N(0, 1)
\end{math} \\[0pt]

\subsection{Poisson Distribution}
\begin{math}
  X \sim Poisson(\lambda),
  f(x) = \frac{e^{-\lambda} \cdot \lambda^x}{x!}
\end{math} \\[2pt]
\begin{math}
  \expect{X} = \lambda,
  \expect{X^2} = \lambda^2 + \lambda,
  Var(X) = \lambda
\end{math} \\[0pt]

\subsection{Exponential Distribution}
\begin{math}
  X \sim Exp(\lambda)
\end{math} \\[2pt]
\begin{math}
  f(x) = \lambda e^{-\lambda x},
  F(x) = 1 - e^{-\lambda x}
\end{math} \\[2pt]
\begin{math}
  \expect{X} = \frac{1}{\lambda},
  \expect{X^2} = \frac{2}{\lambda^2},
  Var(X) = \frac{1}{\lambda^2}
\end{math} \\[2pt]
% (end)
% (fold) Estimation
\section{Estimation}
\begin{math}
  \bar{X} = \frac{1}{n} \sum\limits_{i = 1}^n X_i,
  S^2 = \frac{1}{n - 1} \sum\limits_{i=1}^n (X_i - \bar{X})^2
\end{math} \\[2pt]
\subsection{Central Limit Theorem}
\begin{math}
  \text{If } \expect{X} < \infty \text{ and } Var(X) < \infty \text{ then}
\end{math} \\[2pt]
\begin{math}
  \bar{X} \sim N(\mu, \frac{\sigma^2}{n}) \Rightarrow \frac{\bar{X} - \mu}{\frac{\sigma^2}{n}} \sim N(0, 1)
\end{math} \\[2pt]
\subsection{Confidence Interval}
\begin{math}
  \mu \rightarrow
  Z \sim N(0, 1),
 \bar{X} \pm \cidist{Z} \cdot \frac{\sigma}{\sqrt{n}}
\end{math} \\[2pt]
\begin{math}
  \mu \rightarrow
  T \sim t(n - 1),
 \bar{X} \pm \cidist{T} \cdot \frac{S}{\sqrt{n}}
\end{math} \\[2pt]
\begin{math}
  \sigma^2 \rightarrow
  C \sim \chi^2(n - 1),
  \left[ \frac{(n - 1)S^2}{C_{\frac{\alpha}{2}}}, \frac{(n - 1)S^2}{C_{1 - \frac{\alpha}{2}}} \right]
\end{math} \\[2pt]
% (end)
% (fold) Markov Chains
\section{Markov Chains}
\begin{math}
  P(X_{n+1} = x | X_1 = x_1, X_2 = x_2, \dotsc, X_n = x_n) = P(X_{n + 1} = x | X_n = x_n)
\end{math} \\[2pt]
\begin{math}
  p_{ij}^{(n)} = P(X_n = j | X_0 = i)
\end{math} \\[2pt]
\begin{math}
  \text{State absorbing if }
  p_{ii} = 1 \text{ and } p_{ij} = 0 \text{ for } i \neq j
\end{math} \\[2pt]
\begin{math}
  \text{Probability} = q_j =
  \begin{cases}
    1                       & j = a \\
    0                       & j \neq a \\
    \sum_k P_{jk} \cdot q_k & \text{otherwise} \\
  \end{cases}
\end{math}
\begin{math}
  \text{Steps} = m_j =
  \begin{cases}
    0                       & j \text{ absorbing} \\
    \sum_k P_{jk} \cdot m_k & \text{otherwise} \\
  \end{cases}
\end{math}
% (end)
% (fold) Hypothesis Testing
\section{Hypothesis Testing}
Decide \(H_0\), \(H_0\) True \(\Rightarrow\) OK \\
Decide \(H_0\), \(H_A\) True \(\Rightarrow\) Type II error \\
Decide \(H_A\), \(H_0\) True \(\Rightarrow\) Type I error \\
Decide \(H_A\), \(H_A\) True \(\Rightarrow\) OK \\

\subsection{Test statistics}
\begin{math}
  Z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}},
  t = \frac{\bar{x} - \mu}{S / \sqrt{n}},
  f = \frac{s_1^2}{s_2^2}
\end{math} \\[2pt]
% (end)
% (fold) Generating Functions
\section{Generating Functions}
\begin{math}
  G(a_n; x) = A(x) = \sum_{n = 0}^{\infty} a_n x^n
\end{math} \\[0pt]
\begin{math}
  \left. \frac{\mathrm{d}^nA}{\mathrm{d}x^n} \right|_{x = 0} = a_n \cdot n!
  \Rightarrow
  a_n = \frac{A^{(n)}(0)}{n!}
\end{math} \\[2pt]

\subsection{Common Sequences}
\begin{math}
  a_n = c^n \rightarrow \frac{1}{1 - cx},
  a_k = \binom{n}{k} \rightarrow (1 + x)^n,
  a_n = \binom{k + n}{k} \rightarrow \frac{1}{(1 - x)^{k + 1}}
\end{math} \\[2pt]

\subsection{Common Operations}
\begin{math}
  A(x) + B(x) \rightarrow \left\{ a_n + b_n \right\}_{n = 0}^{\infty}
\end{math} \\[2pt]
\begin{math}
  xA(x) + a_{-1} \rightarrow a_{-1}, a_0, a_1, \dotsc
\end{math} \\[2pt]
\begin{math}
  \frac{A(x) - a_0}{x} \rightarrow a_1, a_2, a_3, \dotsc
\end{math} \\[2pt]
\begin{math}
  A'(x) \rightarrow \left\{ n \cdot a_n \right\}_{n = 0}^{\infty}
\end{math} \\[2pt]
\begin{math}
  A(x) \cdot B(x) \rightarrow c_n = a_0b_n + a_1b_{n - 1} + \dotsb + a_{n - 1}b_1 + a_nb_0
\end{math} \\[2pt]

\subsection{Moment Generating Functions}
\begin{math}
  \expect{X^n} \text{ is the } n\text{-th moment}
\end{math} \\[2pt]
\begin{math}
  \expect{X - \expect{X}}^n \text{ is the } n\text{-th central moment}
\end{math} \\[2pt]
\begin{math}
  m_X(t) = \expect{e^{tX}}
\end{math} \\[2pt]
\begin{math}
  m_X^{(n)}(0) = \expect{X^n}
\end{math} \\[2pt]
\begin{math}
  m_X(t) = m_Y(t) \Rightarrow \text{X, Y equally distributed}
\end{math} \\[2pt]

% (end)
% (fold) Inequalities
\section{Inequalities}
\subsection{Markov's Inequality}
\begin{math}
  X > 0 \text{ and } t > 0 \Rightarrow
  P(X \geq t) \leq \frac{\expect{X}}{t}
\end{math} \\[2pt]

\subsection{Chehbyshev's Inequality}
\begin{math}
  \varepsilon > 0,
  P( \left| X - \mu \right| \geq \varepsilon ) \leq \frac{Var(X)}{\varepsilon^2}
\end{math} \\[2pt]

\subsection{Law of Large Numbers}
\begin{math}
  \lim\limits_{n \rightarrow \infty} \bar{X}_n = \expect{X}
\end{math} \\[2pt]
\begin{math}
  \lim\limits_{n \rightarrow \infty} P( \left| \bar{X}_n - \mu \right| < \varepsilon ) = 1
\end{math} \\[2pt]
% (end)
% (fold) Sequences
\section{Sequences}
\begin{math}
  c^0x^0 + c^1x^1 + c^2x^2 + \dotsb = \sum\limits_{k = 0}^{\infty} (cx)^k = \frac{1}{1 - cx}
\end{math} \\[2pt]
\begin{math}
  \binom{n}{0} x^0 + \binom{n}{1} x^1 + \binom{n}{2} x^2 + \dotsb = \sum\limits_{k = 0}^{\infty} \binom{n}{k} x^k = (1 + x)^n
\end{math} \\[2pt]
\begin{math}
  \binom{k}{k} x^0 + \binom{k + 1}{k} x^1 + \binom{k + 2}{k} x^2 + \dotsb
  = \sum\limits_{n = 0}^{\infty} \binom{n + k}{k} x^n
  = \frac{1}{(1 - x)^{k + 1}}
\end{math} \\[2pt]
% (end)

\newpage

% (fold) Moment Generating Functions
\section{Moment Generating Functions}
\subsection{Geometric Distribution}
\begin{math}
  m_{x}(t) = \sum_{x = 1}^{\infty} e^{tx} (1 - p)^{x - 1}p
           = pe^t \sum_{x = 1}^{\infty}{e^{t(x - 1)}(1 - p)^{x - 1}}
           = pe^t \sum_{x = 0}^{\infty}{e^{tx}(1 - p)^x}
           = pe^t \sum_{x = 0}^{\infty}{(e^t(1 - p))^x}
           = \frac{pe^t}{1 - e^t(1 - p)}
\end{math} \\[2pt]

\subsection{Binomial Distribution}
\begin{math}
  m_{x}(t) = \sum_{x=0}^{n}{e^{tx}\binom{n}{x}p^x(1-p)^{n-x}}
           = \sum_{x=0}^{n}{\binom{n}{x}(pe^t)^x(1-p)^{n-x}}
           = (pe^t + (1 - p))^n = (p(e^t-1)+1)^n
\end{math} \\[2pt]

\subsection{Exponential Distribution}
\begin{math}
  m_{x}(t) = \int_{0}^{\infty}{e^{tx} \lambda e^{-\lambda x}dx}
           = \lambda \int_{0}^{\infty}{e^{x(t-\lambda)}dx}
           = (\text{only if } t < \lambda)
           = \lambda \left[\frac{e^{x(t-\lambda)}}{t-\lambda}\right]_{0}^{\infty}
           = \lambda \left(0-\frac{1}{t-\lambda}\right)
           = \frac{\lambda}{\lambda - t}
\end{math} \\[2pt]

\begin{math}
  F_{x}(x) = P(X \le x) 
           = \int_0^x{f_{x}(s)ds} 
           = \int_0^x{\lambda e^{-\lambda s}ds} 
           = \lambda \left[-\frac{1}{\lambda}e^{-\lambda s} \right ]_0^x 
           = (-1)(e^{-\lambda x} -1) 
           = 1 - e^{-\lambda x}
\end{math} \\[2pt]

\subsection{Poisson Distribution}
\begin{math}
  m_{x}(t) = \sum_{x=0}^{\infty}{e^{tx}}\frac{\lambda^x e^{-\lambda}}{x!}
           = e^{-\lambda}\sum_{x = 0}^{\infty}{\frac{(e^t\lambda)^x}{x!}}
           = (use\ \sum_{x=0}^{\infty}{\frac{\lambda^x}{x!}} = e^\lambda)
           = e^{\lambda(e^t-1)}
\end{math} \\[2pt]
% (end)
% (fold) Inequalitites
\section{Inequalities}
\subsection{Markov}
\begin{math}
  \expect{X} = \int_0^{\infty}{xf_x(x)dx}
             = \int_0^t{xf_x(x)dx} + \int_t^{\infty}{xf_x(x)dx} \ge 0 + \int_t^\infty{tf_x(x)dx}
             = t\int_t^\infty{f_x(x)dx}
             = tP(X \ge t) \Rightarrow \frac{E[X]}{t} \ge P(X \ge t)
\end{math} \\[2pt]

\subsection{Cherbychev}
Use Markov's Inequality:
\begin{math}
  P(|X-\mu| \geq \varepsilon) = P(|X-\mu|^2 \ge \varepsilon^2) \leq \frac{E|X-\mu|^2}{\varepsilon^2}
                              = \frac{Var(X)}{\varepsilon^2}
\end{math} \\[2pt]
% (end)
% (fold) Confidence Intervals
\section{Confidence Intervals}
\begin{math}
  X_1, \dotsc, X_n; X_i \sim N(\mu, \sigma^2)
\end{math} \\[2pt]
\begin{math}
  \text{Use the CTL to get } \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)
\end{math} \\[2pt]
Using that we can construct \(1 - \alpha\) CI intervals.
\begin{math}
  P(-Z_{\frac{\alpha}{2}} \leq \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \leq Z_{\frac{\alpha}{2}}) =
  P(\left| \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \leq Z_{\frac{\alpha}{2}} \right|) =
  P(|\bar{X}-\mu| \le Z_{\frac{\alpha}{2}} \cdot \frac{\sigma}{\sqrt{n}}) =
  P(\bar{X}-Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \le \mu \le \bar{X}+Z_{\frac{\alpha}{2}} \cdot \frac{\sigma}{\sqrt{n}}) =
  1 - \alpha
\end{math} \\[2pt]
% (end)
% (fold) Markov Chains
\section{Markov Chains}
If \(x_n\) has distribution \(\bar{u}\), then \(x_{n+1}\) has distribution \(\bar{u}P\):
\begin{math}
  P(x_{n+1}=j)
           = \sum_i \left[ P(x_n=i) \cdot P(x_{n+1}=j|x_n=i) \right]
           = \sum_i \left[ u_iP_{ij} \right]
           = [\text{matrix} \cdot \text{vector}]
           = \bar{u}Pj
\end{math} \\[2pt]

\subsection{Absorbing Chain}
Assume j not absorbing and let A = \{chain absorbed in a\}:
\begin{math}
  q_j = P(A|x_0 = j)
      = \frac{P(A \cap (x_0 = k))}{P(x_0 = j)}
      = \frac{\sum_{k}{P(A \cap (x_0 = j) \cap (x_1 = k))}}{P(x_0 = j)}
      = \sum_k{\left[P(A|x_0=j \cap x_1 = k) \cdot \frac{P(X_0 = j \cap x_1 = k)}{P(X_0 = j)} \right ]}
      = \sum_k \left[ P(A|x_1 = k) \cdot P(x_1 = k | x_0 = j) \right]
      = \sum_k \left[ P(A|x_1=k) \cdot P_{jk} \right]
      = \sum_k q_kP_{ij}
\end{math} \\[2pt]

% (end)
% (fold) Expectation and Variance
\section{Expectation and Variance}
\begin{math}
  \expect{h(X)} = \sum_k \left[ k \cdot f_{h(X)}(k) \right]
                = \sum_k \left[ k \cdot P(h(X) = k) \right]
                = \sum_k \left[ k \cdot \sum_{x \in \Omega, h(X) = k} \left[ P(X = x) \right] \right]
                = \sum_{x \in \Omega} \left[ \sum_{k, h(X) = k} \left[ k \cdot P(X = x) \right] \right]
                = \sum_{x \in \Omega} \left[ h(x) \cdot P(X = x) \right]
                = \sum_{x \in \Omega} \left[ h(x) \cdot f_x(x) \right]
\end{math} \\[2pt]
\begin{math} \end{math} \\[2pt]
\begin{math}
  \expect{XY} = \sum_x \left[ \sum_y \left[ xy \cdot f_{XY}(x,y) \right] \right]
              = \text{independent}
              = \sum_x \left[ \sum_y \left[ xy \cdot f_Y(x)f_Y(y) \right] \right]
              = \sum_x \left[ xf_X(x) \right] \cdot \sum_y \left[ yf_Y(y) \right]
              = \expect{X} \cdot \expect{Y}
\end{math} \\[2pt]
\begin{math} \end{math} \\[2pt]
\begin{math}
  \expect{X + Y} = \sum_r \left[ r \cdot P(X + Y = r) \right]
                 = \sum_{k,j} \left[ (x_j + y_k) P(X = x_j,Y = y_j) \right]
                 = \sum_{k,j} \left[ x_j \cdot P(X=x_j,Y=y_j) \right] + 
                   \sum_{k,j} \left[ y_k \cdot P(X=x_j,Y=y_j) \right] 
                 = \sum_j \left[ x_j \sum_k \left[ P(X = x_j, Y = y_j) \right] \right] + 
                   \sum_k \left[ y_k \sum_y \left[ P(X = x_j, Y = y_j) \right] \right]
                 = \sum_j \left[ x_j \cdot P(X = x_j) \right] + \sum_k \left[ y_k \cdot P(Y = y_k) \right] 
                 = \expect{X} \expect{Y}
\end{math} \\[2pt]
\begin{math} \end{math} \\[2pt]
\begin{math}
  Var(X) = \expect{X - \expect{X}}^2
         = \expect{X^2 - 2X \expect{X} + \expect{X}^2}
         = \expect{X^2} - 2 \expect{X} \expect{X} + \expect{\expect{X}^2}
         = \expect{X^2} - 2 \expect{X}^2 + \expect{X}^2
         = \expect{X^2} - \expect{X}^2
\end{math} \\[2pt]
\begin{math} \end{math} \\[2pt]
\begin{math}
  Var(X + Y) = \expect{(X + Y)^2} - \expect{X + Y}^2 
             = \expect{X^2 + Y^2 + 2XY} - (\expect{X} + \expect{Y})^2
             = \expect{X^2} - \expect{X}^2 + \expect{Y} - \expect{Y}^2 + 2\expect{XY} - 2 \expect{X} \expect{Y} 
             = Var(X) + Var(Y) + 2Cov(X,Y)
\end{math} \\[2pt]
\begin{math}
  Cov(X,Y) = \expect{(X-\mu_x)(Y-\mu_y)}
           = \expect{XY-X\mu_x-Y\mu_y+\mu_x\mu_y} 
           = \expect{XY}-\mu_y\expect{X}-\mu_x\expect{Y} + \mu_x\mu_y
           = (\expect{X} = \mu_x)
           = \expect{XY} - \expect{X}\expect{Y}
\end{math} \\[2pt]

\section{Other}
\begin{math}
\text{Bayer's:}
  P(A|B) = \frac{P(A\cap B)}{P(B)}\ and\ P(B|A)
         = \frac{P(A\cap B)}{P(A)} 
         \Rightarrow P(A\cap B) = P(A|B)P(B) = P(B|A)P(A)
         \Rightarrow P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{math} \\[2pt]
\begin{math}
\text{marginal distribution:}
  f_x(x) = P(X = x) 
         = P(\bigcup_y \{X=x,Y=y\})
         = \sum_y\left[P(X=x,Y=y) \right ]
         = \sum_y \left[ f_{xy}(x,y) \right ]
\end{math} \\[2pt]
% (end)

\newpage

\end{multicols}
\end{document}
%
% vim: set fdm=marker fmr=(fold),(end) :
